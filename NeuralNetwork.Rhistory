# Attach the MASS package - we will use the Boston dataset
# included in this package
myData = Boston
apply(myData,2,function(x) sum(is.na(x)))

# There are no NAs ... let's just do a summary
summary(myData)
# We'll scale the data - notice the way we do this.
# It gets all values between 0 and 1.

?scale
maxs = apply(myData, 2, max)
mins = apply(myData, 2, min)

# These are vectors of the max and min values for each column
myDataScaled = as.data.frame(scale(myData, center = mins, scale = maxs - mins))

# Let's do a simple CV first to test NN versus Linear Regression
# We'll use 75% of the dataset for training
index = sample(1:nrow(myDataScaled),round(0.75*nrow(myDataScaled)))
train = myDataScaled[index,]
test = myDataScaled[-index,]
lm.fit = glm(medv~.,data=train)
lm.pred = predict(lm.fit,test)
lm.MSE = sum((lm.pred-test$medv)^2)/nrow(test)
lm.MSE

# Remember this is scaled, so we would need to scale back.

# Let's try Lasso next. We need to feed in X, Y separately ...
# We can get these from myData by using the 'subset' function...

X = subset(myDataScaled,select=c(-medv))
Y = myDataScaled$medv

# Attach glmnet ...
lasso.mod=glmnet(X,Y,alpha=1)

# oops - not happy. It needs X to be a matrix ...
Xmat = as.matrix(X)
lasso.mod=glmnet(Xmat,Y,alpha=1)

# We actually only need the cv output ...
cv.out=cv.glmnet(Xmat,Y,alpha=1)
min(cv.out$cvm)
# In line with Linear Regression ...

# Now let's build a simple ANN. Attach the 'neuralnet' package
?neuralnet

# We will specify a simple network with one hidden layer
# containing 6 nodes. Also, linear.output to TRUE
# since we are not doing logistic regression.
nn.fit = neuralnet(medv~.,data=train,hidden=c(6),linear.output=TRUE)

# Oops ... nerualnet wants us to explicitly write out the
# formula! Short cut ...
n = names(train)
n
f = as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
f
nn.fit = neuralnet(f,data=train,hidden=c(6),linear.output=TRUE)
plot(nn.fit)

# Cool - how did it do?
nn.pred = compute(nn.fit,test[,1:13])
names(nn.pred)
nn.MSE = sum((test$medv - nn.pred$net.result)^2)/nrow(test)
nn.MSE

# Comparable. Let's recale and see how accurate this is
nn.predUnscale = nn.pred$net.result*(max(myData$medv)-min(myData$medv))+min(myData$medv)
testMedvUnscale = (test$medv)*(max(myData$medv)-min(myData$medv))+min(myData$medv)
MSEUnscaled = sum((testMedvUnscale - nn.predUnscale)^2)/nrow(test)
sqrt(MSEUnscaled)
summary(myData$medv)
sqrt(MSEUnscaled)/mean(myData$medv)

# Now let's try 10-fold CV to see if we get a better result.
# Attach the 'boot' package.
lm.fit = glm(medv~., data=myDataScaled)
summary(lm.fit)
cv.glm(myDataScaled, lm.fit, K=10)$delta[1]

# Now we'll do something similar for the ANN - first, we load
# a helpful package that will produce a progress bar - 'plyr'
bins = sample(1:10,nrow(myDataScaled), replace = TRUE)
binErrs = rep(0,10)
pbar = create_progress_bar('text')
pbar$init(10)
for(i in 1:10){
  train.cv = myDataScaled[bins != i,]
  test.cv = myDataScaled[bins == i,]
  nn.fit = neuralnet(f,data=train.cv,hidden=c(6),linear.output=TRUE)
  nn.pred = compute(nn.fit,test.cv[,1:13])
  binErrs[i] <- sum((test.cv$medv - nn.pred$net.result)^2)/nrow(test.cv)
  pbar$step()
}
mean(binErrs)

# Let's go back to our simple CV and try adding in more layers
nn.fit = neuralnet(f,data=train,hidden=c(12,6),linear.output=TRUE)
nn.pred = compute(nn.fit,test[,1:13])
nn.MSE_2 = sum((test$medv - nn.pred$net.result)^2)/nrow(test)
nn.MSE
nn.MSE_2
