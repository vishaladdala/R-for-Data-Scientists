# We will first work with the spirit weight dataset -
# read this in.
View(spiritwt_data)
colnames(spiritwt_data)=c("Temp","Dilution","Wght")
attach(spiritwt_data)

# We'll build a simple linear regression model first
lr.model_0 = lm(Wght~Dilution)
summary(lr.model_0)

# Now let's add in both terms ...
lr.model_1 = lm(Wght~Temp+Dilution)
summary(lr.model_1)

# Much improved over single variable (R^2)
plot(lr.model_1$fitted.values,lr.model_1$residuals)
plot(Temp, lr.model_1$residuals)
plot(Dilution,lr.model_1$residuals)

# Looks like there is nonlinearity in the dilution variable
lr.model_2 = lm(Wght~Temp+poly(Dilution,2))
summary(lr.model_2)

# All p-values small, good sign ...
plot(Dilution,lr.model_2$residuals)

# Still non-linear effect ...
lr.model_4 = lm(Wght~Temp+poly(Dilution,4))
summary(lr.model_4)
plot(Dilution,lr.model_4$residuals)
qqnorm(lr.model_4$residuals)
qqline(lr.model_4$residuals)

# It may be that the variance is not constant ...
lr.model_5 = lm(Wght~Temp+poly(Dilution,5))
summary(lr.model_5)
plot(Dilution,lr.model_5$residuals)

# Another way to introduce nonlinearity (gently) is with a
# cross term.
lr.model_cross = lm(Wght~Temp+Dilution+Temp*Dilution)
summary(lr.model_cross)

# The p-vlaue is too high, this term is probably not significant
# There is probably no interaction between these two predictors


# Next read in the pgalpga2008 dataset ...
View(pgalpga2008)
attach(pgalpga2008)
summary(pgalpga2008)
pgalpga2008$Gender = as.factor((pgalpga2008$Gender))
summary(pgalpga2008)
plot(Distance~Accuracy, col=Gender)

# The gender predictor is probably significant ...
lr.model_3 = lm(Accuracy~Distance+Gender)
summary(lr.model_3)

# Not a great fit, let's check the residuals ...
plot(lr.model_3$residuals~Distance)

# Not bad, there are some outliers
boxplot(Accuracy)
boxplot(Distance)
plot(lr.model_3$residuals~Gender)

# This plot makes less sense, since Gender is a factor
# The low R^2 may indicate that there are other factors in
# play that are not captured in our data (and hence model)

# Now read in the Concrete_Data_Tab dataset
View(Concrete_Data_Tab)
summary(Concrete_Data_Tab)
lr.model.concrete=lm(compressivestrength~., data=Concrete_Data_Tab)
summary(lr.model.concrete)

# Here we can probably remove the Aggregate predictors
# We can then check residuals ...
myData = Concrete_Data_Tab
myData$`Fine Aggregate`=NULL
myData$CoarseAggregate=NULL
attach(myData)
lr.model.concrete=lm(compressivestrength~., data=myData)
summary(lr.model.concrete)
plot(lr.model.concrete$residuals~lr.model.concrete$fitted.values)
qqnorm(lr.model.concrete$residuals)
qqline(lr.model.concrete$residuals)

# Looks like a good fit, even though R^2 i a bit high.
plot(lr.model.concrete$residuals~Cement)

# Next let's look at data for birthweight of babies
View(lowbwt)
summary(lowbwt)

# We can remove the ID and the Low indicator (this is what
# we are trying to predict)
myData = lowbwt[,-1]
myData = myData[,-1]
names(myData)

# Several of these predictors are categorical
# Can check the data dictionary to be sure
myData$RACE=as.factor(myData$RACE)
myData$SMOKE=as.factor(myData$SMOKE)
myData$PTL=as.factor(myData$PTL)
myData$HT=as.factor(myData$HT)
myData$UI=as.factor(myData$UI)
myData$FTV=as.factor(myData$FTV)
summary(myData)

# Note that for PTL and FTV, we should re-define the categories
# to remove the ones with small frequencies
myData$PTL = as.numeric(as.character(myData$PTL))
myData$FTV = as.numeric(as.character(myData$FTV))
myData$PTL = ifelse(myData$PTL>1,1,myData$PTL)
myData$FTV = ifelse(myData$FTV>2,2,myData$FTV)
summary(myData)
myData$PTL=as.factor(myData$PTL)
myData$FTV=as.factor(myData$FTV)
summary(myData)
lr.model_BWT = lm(BWT~., data=myData)
summary(lr.model_BWT)
lr.model_BWT = lm(BWT~LWT+RACE+SMOKE+HT+UI, data=myData)
summary(lr.model_BWT)

# Notice the model does not appear to do a good job of
# predicting birth weight, but this may not be what we
# want to do anyway - perhaps we can build a model that
# predicts LOW bwt - classfication is usually easier!
plot(lr.model_BWT$residuals~lr.model_BWT$fitted.values)
qqnorm(lr.model_BWT$residuals)

# There are probably other factors invovled with predicing
# weight
# Next we look at Car Worth data ...
View(Car.Worth)
attach(Car.Worth)
summary(Car.Worth)

# There are several numerical predictors that should be
# converted to factors
myData=Car.Worth
myData$Cylinder=as.factor(myData$Cylinder)
myData$Doors=as.factor(myData$Doors)
myData$Cruise=as.factor(myData$Cruise)
myData$Sound=as.factor(myData$Sound)
myData$Leather=as.factor(myData$Leather)
summary(myData)

# Categorical predictors with many categories are going to be
# a problem - too many dummy variables. Model probably already
# covered by Make. Drop trim too.
attach(myData)
plot(Make~Model)

lr.model_cards = lm(Price~. -Model -Trim, data=myData)
summary(lr.model_cards)

# Doors is colinear with another predictor. Also, Leather
# sound and cruise do not seem to add much.
lr.model_cars2 = update(lr.model_cards, .~.-Doors -Leather -Sound -Cruise)
summary(lr.model_cars2)

# Not bad - let's check the residuals
plot(lr.model_cars2$residuals~lr.model_cars2$fitted.values)

# There are some outliers in the high end range
myData_2 = myData[myData$Price<=50000,]
lr.model_cars_3 = lm(Price ~ . -Model -Trim -Doors -Leather -Sound -Cruise, data=myData_2)
summary(lr.model_cars_3)
plot(lr.model_cars_3$residuals~lr.model_cars_3$fitted.values)

# Model probably works better for lower priced cars, variance
# is larger for higher priced models (note there is no
# predictor for car condition, probably more important for
# high priced cars)
# Scaling the price may produce a better result ...
myData_2$Price=log(myData_2$Price)
summary(myData_2)
lr.model_cars_3 = lm(Price ~ . -Model -Trim -Doors -Leather -Sound -Cruise, data=myData_2)
plot(lr.model_cars_3$residuals~lr.model_cars_3$fitted.values)

# Finally, we take a look at some data for cigarette brands
View(cigarettes.data)
summary(cigarettes.data)
myData = cigarettes.data[,-1]
boxplot(myData)

# We don't need the names any way, so we may want to
# remove the outliers in Tar and CO and work with myData ...
myData_2 = myData[myData$Tar<25,]
myData_3 = myData_2[myData_2$CO>5,]
boxplot(myData_3)

# We will try to model CO in terms of the other predictors
lr.model.cig = lm(CO~., data=myData_3)
summary(lr.model.cig)
plot(myData_3)

# Here we see Weight is probably not signficant, and
# Tar and Nicotine are correlated, so we only really need one ...
lr.model.cig2 = lm(CO~Tar, data=myData_3)
summary(lr.model.cig2)
plot(lr.model.cig2$residuals~myData_3$Tar)

# Small data set, but looks good ...
qqnorm(lr.model.cig2$residuals)
qqline(lr.model.cig2$residuals)
