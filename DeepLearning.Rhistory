# Now we will look at another package - mxnet. First,
# install and attach the 'drat' package. Then issue 
# the following commands ...
#	cran <- getOption("repos")
#	cran["dmlc"] <- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"
#	options(repos = cran)
#	install.packages("mxnet")
#	install.packages("viridisLite")
#	install.packages("devtools")
#	require(devtools)
#	install_version("DiagrammeR", version = "0.9.0", repos = "http://cran.us.r-project.org")
#	require(DiagrammeR)

# We'll do a regression model next. Attach MASS - 
# we will work with Boston as before.

summary(Boston)
dim(Boston)

myData = Boston
maxs = apply(myData, 2, max)
mins = apply(myData, 2, min)
myDataScaled = as.data.frame(scale(myData, center = mins, scale = maxs - mins))

train.ind = sample(506,400)
train.x = data.matrix(myDataScaled [train.ind, -14])
train.y = myDataScaled [train.ind, 14]
test.x = data.matrix(myDataScaled [-train.ind, -14])
test.y = myDataScaled [-train.ind, 14]
train.x = scale(train.x)
test.x = scale(test.x)

# MXNet actually allows us to build networks in a general
# manner ... Define the input data
data <- mx.symbol.Variable("data")

# A fully connected hidden layer
# data: input source
# num_hidden: number of neurons in this hidden layer
fc1 <- mx.symbol.FullyConnected(data, num_hidden=6)
act1 <- mx.symbol.Activation(fc1, act_type="relu")
fc2 <- mx.symbol.FullyConnected(act1, num_hidden=1)


# Use linear regression for the output layer
lro <- mx.symbol.LinearRegressionOutput(fc2)

# Here we create the model.
model <- mx.model.FeedForward.create(lro, X=train.x, y=train.y,
ctx=mx.cpu(),num.round=50, array.batch.size=20,
learning.rate=.1, momentum=0.9,  eval.metric=mx.metric.rmse)
graph.viz(model$symbol)

# Let's check the accuracy ...
preds = predict(model, test.x)
sqrt(mean((preds-test.y)^2))
summary(BostonHousing[,14])

# Note that MXNet also allows you to plug in your own custom
# metric ... See tutorial for details

# Now we will look at one more dataset. We'll use
# the sonar dataset in the 'mlbench' package.
data(Sonar, package="mlbench")
summary(Sonar)

# We need out response to be numeric, even though this is
# a classification problem. 
Sonar[,61] = as.numeric(Sonar[,61])-1
dim(Sonar)
train = sample(208,150)
train.x = data.matrix(Sonar[train, 1:60])
train.y = Sonar[train, 61]
test.x = data.matrix(Sonar[-train, 1:60])
test.y = Sonar[-train, 61]

# Here we build the model - we build one layer at a time
data <- mx.symbol.Variable("data")
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=10)
act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")
fc2 <- mx.symbol.FullyConnected(act1, name="fc3", num_hidden=2)
softmax <- mx.symbol.SoftmaxOutput(fc2, name="sm")
devices = mx.cpu()

model <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y,
ctx=devices, num.round=40, array.batch.size=100,
learning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy,
initializer=mx.init.uniform(0.07),
epoch.end.callback=mx.callback.log.train.metric(100))

# This command gives us a graphical representation of our model
graph.viz(model$symbol)

# Let's see how it did 
preds = predict(model, test.x)
pred.label = max.col(t(preds))-1
table(pred.label, test.y)
mean(test.y == pred.label)