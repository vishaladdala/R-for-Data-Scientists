# Let's work with the USArrests data first
pca_arrests = prcomp(USArrests,scale. = TRUE, center = TRUE)
biplot(pca_arrests,cex=c(1/3,1/2), scale=0)

# Let's plot the PVE
plot(pca_arrests)
names(pca_arrests)

# sdev gives the SD explained by each PC
pca_arrests$sdev

# We can compute the PVE as follows:
pca_arrests.var =pca_arrests$sdev ^2
pve=pca_arrests.var/sum(pca_arrests.var )
pve

# Here's the plot ...
plot(pve , xlab=" Principal Component ", ylab=" Proportion of
     Variance Explained ", ylim=c(0,1) ,type='b')

# Another view ...
plot(cumsum (pve ), xlab=" Principal Component ", ylab ="
     Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
     type='b')
# Two components seems to be enough ...

# Let's return to the student eval data ...
# Read in the Turkey student data ...

# Clean up the data - remove first column
myData = turkiye.student.evaluation_R_Specific[-1]

# Here we do PCA to see how many dimensions we really need
pca_studentEval=prcomp(myData, scale. = TRUE, center = TRUE)
plot(pca_studentEval)
pca_studentEval.var =pca_studentEval$sdev ^2
pve=pca_studentEval.var/sum(pca_studentEval.var )
plot(pve , xlab=" Principal Component ", ylab=" Proportion of
     Variance Explained ", ylim=c(0,1) ,type='b')
plot(cumsum (pve ), xlab=" Principal Component ", ylab ="
     Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
     type='b')

# Recall that there was a column in pca_arrests called 'x'
# This is actually the coordinates of the projected data
dim(pca_arrests$x)

# Let's do an H-Cluster using only the first two PCs
pca_arrests.scaled = scale(pca_arrests$x[,1:2])
pca_arrests.ward = hclust(dist(pca_arrests.scaled), method="ward.D2")
plot(pca_arrests.ward,main="Ward Linkage", xlab="", sub="", cex=.9)
cutree(pca_arrests.ward,4)

# We'll look at Stone Flakes again ...
# Load in the Stone Flakes dataset ...
summary(StoneFlakes)

myData$FLA=as.numeric(myData$FLA)
myData$PSF=as.numeric(myData$PSF)
myData$FSF=as.numeric(myData$FSF)
myData$ZDF1=as.numeric(myData$ZDF1)

# Clean up ...
myData = na.omit(StoneFlakes)
myData = scale(myData[-1])

# Let's find the optimal number of clusters using K-Means
# and the Elbow method
k.max <- 15
data <- myData
wss <- sapply(2:k.max,
              function(k){kmeans(data, k, nstart=10 )$tot.withinss})
plot(2:k.max, wss,
     type="b", pch = 19, frame = FALSE,
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

# No real 'elbow', but we'll use 4.

km_stones = kmeans(myData,4)
km_stones
# betweenSS/TotalSS is close to 50%

# Now let's use PCA and see if there is an improvement ...

pca_stones = prcomp(myData, scale. = TRUE, center = TRUE)
pca_stones.var =pca_stones$sdev ^2
pve=pca_stones.var/sum(pca_stones.var )
plot(pve , xlab=" Principal Component ", ylab=" Proportion of
     Variance Explained ", ylim=c(0,1) ,type='b')

# Let's use 3 PCs ...
km_stones_pca = kmeans(pca_stones$x[,1:3],4)
km_stones_pca
# We increased the betweenSS/totalSS to over 60%

# Finally, let's return to the Iris data
# Recall our earlier cluster analysis ...
newiris <- iris
summary(newiris)
newiris$Species <- NULL
kc <- kmeans(newiris, 3)
table(iris$Species, kc$cluster)

# We did not scale the data, but it probably does not matter ...
iris.scaled = scale(iris[,-5])
kc <- kmeans(iris.scaled, 3)
table(iris$Species, kc$cluster)

# Recall we saw a better result when we only used a couple of variables ...
kc_reduced <- kmeans(iris.scaled[,3:4], 3)
table(iris$Species, kc_reduced$cluster)
summary(iris.scaled)

# Let's apply PCA ...
pca_iris = prcomp(iris.scaled)
biplot(pca_iris,cex=c(1/3,1/2), scale=0)
pca_iris.var =pca_iris$sdev ^2
pve=pca_iris.var/sum(pca_iris.var)
plot(cumsum (pve ), xlab=" Principal Component ", ylab ="
     Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
     type='b')
km_iris_pca = kmeans(pca_iris$x[,1:2],3)
table(iris$Species, km_iris_pca$cluster)

# Surprisingly, does not seem to help. But ...

km_iris_pca = kmeans(pca_iris$x[,1],3)
table(iris$Species, km_iris_pca$cluster)

# Much better!

